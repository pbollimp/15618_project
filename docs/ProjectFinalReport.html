<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=yrPxF2fzIbKO47OcVFfZ2CvhBULzVSj67-CyYQmQBzclLT34rKZNjX_x5wwcqB1FJcp4QFgPPqKvq4Bp_6u4rQ');.lst-kix_50lonl88s381-0>li:before{content:"\0025cf  "}.lst-kix_4uimhjzd4p6t-8>li{counter-increment:lst-ctn-kix_4uimhjzd4p6t-8}ol.lst-kix_4uimhjzd4p6t-1.start{counter-reset:lst-ctn-kix_4uimhjzd4p6t-1 0}.lst-kix_50lonl88s381-3>li:before{content:"\0025cf  "}.lst-kix_50lonl88s381-2>li:before{content:"\0025a0  "}.lst-kix_50lonl88s381-1>li:before{content:"\0025cb  "}.lst-kix_50lonl88s381-8>li:before{content:"\0025a0  "}ol.lst-kix_rvalvqgswccz-7.start{counter-reset:lst-ctn-kix_rvalvqgswccz-7 0}.lst-kix_50lonl88s381-7>li:before{content:"\0025cb  "}ol.lst-kix_7xxmatpvz3rf-8.start{counter-reset:lst-ctn-kix_7xxmatpvz3rf-8 0}.lst-kix_50lonl88s381-4>li:before{content:"\0025cb  "}.lst-kix_mdcgjkjfaoe6-7>li{counter-increment:lst-ctn-kix_mdcgjkjfaoe6-7}.lst-kix_50lonl88s381-6>li:before{content:"\0025cf  "}.lst-kix_50lonl88s381-5>li:before{content:"\0025a0  "}.lst-kix_km8gilkue97c-5>li{counter-increment:lst-ctn-kix_km8gilkue97c-5}ol.lst-kix_gnobxi7p2flf-6{list-style-type:none}.lst-kix_gnobxi7p2flf-0>li{counter-increment:lst-ctn-kix_gnobxi7p2flf-0}ol.lst-kix_gnobxi7p2flf-5{list-style-type:none}ol.lst-kix_gnobxi7p2flf-8{list-style-type:none}ol.lst-kix_gnobxi7p2flf-3.start{counter-reset:lst-ctn-kix_gnobxi7p2flf-3 0}ol.lst-kix_gnobxi7p2flf-7{list-style-type:none}ol.lst-kix_4firhkley64h-0.start{counter-reset:lst-ctn-kix_4firhkley64h-0 0}.lst-kix_3tailnsur1sm-3>li{counter-increment:lst-ctn-kix_3tailnsur1sm-3}.lst-kix_nylkdundlnhm-7>li{counter-increment:lst-ctn-kix_nylkdundlnhm-7}.lst-kix_9nztxlv7ylzk-7>li{counter-increment:lst-ctn-kix_9nztxlv7ylzk-7}ol.lst-kix_9nztxlv7ylzk-6.start{counter-reset:lst-ctn-kix_9nztxlv7ylzk-6 0}ol.lst-kix_gnobxi7p2flf-2{list-style-type:none}ol.lst-kix_nylkdundlnhm-1.start{counter-reset:lst-ctn-kix_nylkdundlnhm-1 0}ol.lst-kix_gnobxi7p2flf-1{list-style-type:none}ol.lst-kix_3tailnsur1sm-0.start{counter-reset:lst-ctn-kix_3tailnsur1sm-0 0}ol.lst-kix_gnobxi7p2flf-4{list-style-type:none}ol.lst-kix_gnobxi7p2flf-3{list-style-type:none}ol.lst-kix_gnobxi7p2flf-0{list-style-type:none}.lst-kix_rvalvqgswccz-8>li{counter-increment:lst-ctn-kix_rvalvqgswccz-8}.lst-kix_7xxmatpvz3rf-4>li{counter-increment:lst-ctn-kix_7xxmatpvz3rf-4}ol.lst-kix_4uimhjzd4p6t-6.start{counter-reset:lst-ctn-kix_4uimhjzd4p6t-6 0}.lst-kix_7xxmatpvz3rf-1>li:before{content:"" counter(lst-ctn-kix_7xxmatpvz3rf-1,lower-latin) ". "}.lst-kix_gnobxi7p2flf-0>li:before{content:"" counter(lst-ctn-kix_gnobxi7p2flf-0,decimal) ") "}.lst-kix_gnobxi7p2flf-4>li{counter-increment:lst-ctn-kix_gnobxi7p2flf-4}.lst-kix_4firhkley64h-3>li{counter-increment:lst-ctn-kix_4firhkley64h-3}.lst-kix_7xxmatpvz3rf-0>li:before{content:"" counter(lst-ctn-kix_7xxmatpvz3rf-0,decimal) ". "}.lst-kix_gnobxi7p2flf-1>li:before{content:"" counter(lst-ctn-kix_gnobxi7p2flf-1,lower-latin) ") "}.lst-kix_gnobxi7p2flf-2>li:before{content:"" counter(lst-ctn-kix_gnobxi7p2flf-2,lower-roman) ") "}ol.lst-kix_mdcgjkjfaoe6-2.start{counter-reset:lst-ctn-kix_mdcgjkjfaoe6-2 0}.lst-kix_rvalvqgswccz-4>li{counter-increment:lst-ctn-kix_rvalvqgswccz-4}.lst-kix_nylkdundlnhm-3>li{counter-increment:lst-ctn-kix_nylkdundlnhm-3}.lst-kix_rvalvqgswccz-1>li{counter-increment:lst-ctn-kix_rvalvqgswccz-1}ol.lst-kix_4firhkley64h-5.start{counter-reset:lst-ctn-kix_4firhkley64h-5 0}.lst-kix_7xxmatpvz3rf-0>li{counter-increment:lst-ctn-kix_7xxmatpvz3rf-0}ol.lst-kix_nylkdundlnhm-6.start{counter-reset:lst-ctn-kix_nylkdundlnhm-6 0}ol.lst-kix_7xxmatpvz3rf-1.start{counter-reset:lst-ctn-kix_7xxmatpvz3rf-1 0}.lst-kix_nylkdundlnhm-0>li{counter-increment:lst-ctn-kix_nylkdundlnhm-0}.lst-kix_4uimhjzd4p6t-4>li:before{content:"" counter(lst-ctn-kix_4uimhjzd4p6t-4,lower-latin) ". "}.lst-kix_4firhkley64h-7>li{counter-increment:lst-ctn-kix_4firhkley64h-7}.lst-kix_cgm9v8hzfvyl-7>li:before{content:"\0025cb  "}ul.lst-kix_mt7nux36tyir-8{list-style-type:none}.lst-kix_3tailnsur1sm-7>li{counter-increment:lst-ctn-kix_3tailnsur1sm-7}ul.lst-kix_mt7nux36tyir-7{list-style-type:none}.lst-kix_4uimhjzd4p6t-2>li:before{content:"" counter(lst-ctn-kix_4uimhjzd4p6t-2,lower-roman) ". "}.lst-kix_4uimhjzd4p6t-6>li:before{content:"" counter(lst-ctn-kix_4uimhjzd4p6t-6,decimal) ". "}.lst-kix_4uimhjzd4p6t-0>li:before{content:"" counter(lst-ctn-kix_4uimhjzd4p6t-0,decimal) ". "}.lst-kix_4uimhjzd4p6t-8>li:before{content:"" counter(lst-ctn-kix_4uimhjzd4p6t-8,lower-roman) ". "}ul.lst-kix_mt7nux36tyir-2{list-style-type:none}ol.lst-kix_rvalvqgswccz-0.start{counter-reset:lst-ctn-kix_rvalvqgswccz-0 0}ul.lst-kix_mt7nux36tyir-1{list-style-type:none}.lst-kix_cgm9v8hzfvyl-3>li:before{content:"\0025cf  "}.lst-kix_cgm9v8hzfvyl-5>li:before{content:"\0025a0  "}ul.lst-kix_mt7nux36tyir-0{list-style-type:none}ul.lst-kix_mt7nux36tyir-6{list-style-type:none}ul.lst-kix_mt7nux36tyir-5{list-style-type:none}ul.lst-kix_mt7nux36tyir-4{list-style-type:none}ul.lst-kix_mt7nux36tyir-3{list-style-type:none}.lst-kix_cgm9v8hzfvyl-1>li:before{content:"\0025cb  "}.lst-kix_mdcgjkjfaoe6-0>li{counter-increment:lst-ctn-kix_mdcgjkjfaoe6-0}.lst-kix_gnobxi7p2flf-7>li{counter-increment:lst-ctn-kix_gnobxi7p2flf-7}.lst-kix_gnobxi7p2flf-8>li{counter-increment:lst-ctn-kix_gnobxi7p2flf-8}.lst-kix_3tailnsur1sm-6>li{counter-increment:lst-ctn-kix_3tailnsur1sm-6}.lst-kix_4uimhjzd4p6t-1>li{counter-increment:lst-ctn-kix_4uimhjzd4p6t-1}.lst-kix_gnobxi7p2flf-6>li:before{content:"" counter(lst-ctn-kix_gnobxi7p2flf-6,decimal) ". "}.lst-kix_km8gilkue97c-4>li:before{content:"" counter(lst-ctn-kix_km8gilkue97c-4,lower-latin) ". "}.lst-kix_km8gilkue97c-6>li:before{content:"" counter(lst-ctn-kix_km8gilkue97c-6,decimal) ". "}.lst-kix_7xxmatpvz3rf-7>li:before{content:"" counter(lst-ctn-kix_7xxmatpvz3rf-7,lower-latin) ". "}.lst-kix_gnobxi7p2flf-4>li:before{content:"(" counter(lst-ctn-kix_gnobxi7p2flf-4,lower-latin) ") "}.lst-kix_gnobxi7p2flf-8>li:before{content:"" counter(lst-ctn-kix_gnobxi7p2flf-8,lower-roman) ". "}.lst-kix_km8gilkue97c-8>li:before{content:"" counter(lst-ctn-kix_km8gilkue97c-8,lower-roman) ". "}.lst-kix_km8gilkue97c-0>li:before{content:"" counter(lst-ctn-kix_km8gilkue97c-0,decimal) ". "}.lst-kix_7xxmatpvz3rf-3>li:before{content:"" counter(lst-ctn-kix_7xxmatpvz3rf-3,decimal) ". "}ul.lst-kix_cgm9v8hzfvyl-0{list-style-type:none}.lst-kix_4uimhjzd4p6t-0>li{counter-increment:lst-ctn-kix_4uimhjzd4p6t-0}ul.lst-kix_cgm9v8hzfvyl-1{list-style-type:none}.lst-kix_rvalvqgswccz-5>li{counter-increment:lst-ctn-kix_rvalvqgswccz-5}ol.lst-kix_7xxmatpvz3rf-6.start{counter-reset:lst-ctn-kix_7xxmatpvz3rf-6 0}ul.lst-kix_cgm9v8hzfvyl-8{list-style-type:none}ul.lst-kix_cgm9v8hzfvyl-6{list-style-type:none}.lst-kix_km8gilkue97c-2>li:before{content:"" counter(lst-ctn-kix_km8gilkue97c-2,lower-roman) ". "}ul.lst-kix_cgm9v8hzfvyl-7{list-style-type:none}.lst-kix_7xxmatpvz3rf-5>li:before{content:"" counter(lst-ctn-kix_7xxmatpvz3rf-5,lower-roman) ". "}ul.lst-kix_cgm9v8hzfvyl-4{list-style-type:none}ul.lst-kix_cgm9v8hzfvyl-5{list-style-type:none}ul.lst-kix_cgm9v8hzfvyl-2{list-style-type:none}ul.lst-kix_cgm9v8hzfvyl-3{list-style-type:none}.lst-kix_rvalvqgswccz-0>li{counter-increment:lst-ctn-kix_rvalvqgswccz-0}ol.lst-kix_nylkdundlnhm-3.start{counter-reset:lst-ctn-kix_nylkdundlnhm-3 0}ol.lst-kix_7xxmatpvz3rf-3.start{counter-reset:lst-ctn-kix_7xxmatpvz3rf-3 0}ol.lst-kix_4uimhjzd4p6t-1{list-style-type:none}ol.lst-kix_4uimhjzd4p6t-0{list-style-type:none}ol.lst-kix_4uimhjzd4p6t-3{list-style-type:none}ol.lst-kix_4uimhjzd4p6t-2{list-style-type:none}ol.lst-kix_rvalvqgswccz-2.start{counter-reset:lst-ctn-kix_rvalvqgswccz-2 0}ol.lst-kix_gnobxi7p2flf-1.start{counter-reset:lst-ctn-kix_gnobxi7p2flf-1 0}ol.lst-kix_4uimhjzd4p6t-8{list-style-type:none}ol.lst-kix_4uimhjzd4p6t-5{list-style-type:none}ol.lst-kix_4uimhjzd4p6t-4{list-style-type:none}ol.lst-kix_4uimhjzd4p6t-7{list-style-type:none}ol.lst-kix_4uimhjzd4p6t-6{list-style-type:none}ol.lst-kix_9nztxlv7ylzk-8.start{counter-reset:lst-ctn-kix_9nztxlv7ylzk-8 0}.lst-kix_3tailnsur1sm-8>li:before{content:"" counter(lst-ctn-kix_3tailnsur1sm-8,lower-roman) ". "}.lst-kix_9ykvx5ecj10g-7>li:before{content:"\0025cb  "}ol.lst-kix_4firhkley64h-2.start{counter-reset:lst-ctn-kix_4firhkley64h-2 0}.lst-kix_3tailnsur1sm-6>li:before{content:"" counter(lst-ctn-kix_3tailnsur1sm-6,decimal) ". "}.lst-kix_4firhkley64h-2>li{counter-increment:lst-ctn-kix_4firhkley64h-2}.lst-kix_3tailnsur1sm-4>li:before{content:"(" counter(lst-ctn-kix_3tailnsur1sm-4,lower-latin) ") "}.lst-kix_9ykvx5ecj10g-3>li:before{content:"\0025cf  "}ol.lst-kix_4uimhjzd4p6t-3.start{counter-reset:lst-ctn-kix_4uimhjzd4p6t-3 0}.lst-kix_9ykvx5ecj10g-1>li:before{content:"\0025cb  "}.lst-kix_9ykvx5ecj10g-5>li:before{content:"\0025a0  "}.lst-kix_9nztxlv7ylzk-0>li{counter-increment:lst-ctn-kix_9nztxlv7ylzk-0}.lst-kix_3tailnsur1sm-2>li:before{content:"" counter(lst-ctn-kix_3tailnsur1sm-2,lower-roman) ") "}.lst-kix_km8gilkue97c-4>li{counter-increment:lst-ctn-kix_km8gilkue97c-4}.lst-kix_nylkdundlnhm-8>li{counter-increment:lst-ctn-kix_nylkdundlnhm-8}.lst-kix_3tailnsur1sm-0>li:before{content:"" counter(lst-ctn-kix_3tailnsur1sm-0,decimal) ") "}ol.lst-kix_mdcgjkjfaoe6-0.start{counter-reset:lst-ctn-kix_mdcgjkjfaoe6-0 0}ol.lst-kix_km8gilkue97c-2.start{counter-reset:lst-ctn-kix_km8gilkue97c-2 0}ol.lst-kix_3tailnsur1sm-3.start{counter-reset:lst-ctn-kix_3tailnsur1sm-3 0}.lst-kix_mdcgjkjfaoe6-8>li:before{content:"" counter(lst-ctn-kix_mdcgjkjfaoe6-8,lower-roman) ". "}.lst-kix_gnobxi7p2flf-1>li{counter-increment:lst-ctn-kix_gnobxi7p2flf-1}ol.lst-kix_4uimhjzd4p6t-4.start{counter-reset:lst-ctn-kix_4uimhjzd4p6t-4 0}ol.lst-kix_7xxmatpvz3rf-5.start{counter-reset:lst-ctn-kix_7xxmatpvz3rf-5 0}ol.lst-kix_9nztxlv7ylzk-8{list-style-type:none}ol.lst-kix_9nztxlv7ylzk-7{list-style-type:none}.lst-kix_lhvf5ffpxyq-8>li:before{content:"\0025a0  "}ol.lst-kix_9nztxlv7ylzk-0{list-style-type:none}.lst-kix_km8gilkue97c-6>li{counter-increment:lst-ctn-kix_km8gilkue97c-6}.lst-kix_lhvf5ffpxyq-5>li:before{content:"\0025a0  "}ol.lst-kix_9nztxlv7ylzk-2{list-style-type:none}ol.lst-kix_9nztxlv7ylzk-1{list-style-type:none}ol.lst-kix_9nztxlv7ylzk-4{list-style-type:none}ol.lst-kix_9nztxlv7ylzk-3{list-style-type:none}ol.lst-kix_9nztxlv7ylzk-6{list-style-type:none}.lst-kix_3tailnsur1sm-0>li{counter-increment:lst-ctn-kix_3tailnsur1sm-0}ol.lst-kix_9nztxlv7ylzk-5{list-style-type:none}.lst-kix_lhvf5ffpxyq-1>li:before{content:"\0025cb  "}.lst-kix_lhvf5ffpxyq-0>li:before{content:"\0025cf  "}.lst-kix_lhvf5ffpxyq-4>li:before{content:"\0025cb  "}ol.lst-kix_mdcgjkjfaoe6-6.start{counter-reset:lst-ctn-kix_mdcgjkjfaoe6-6 0}ol.lst-kix_km8gilkue97c-8.start{counter-reset:lst-ctn-kix_km8gilkue97c-8 0}.lst-kix_4firhkley64h-4>li{counter-increment:lst-ctn-kix_4firhkley64h-4}ol.lst-kix_rvalvqgswccz-5.start{counter-reset:lst-ctn-kix_rvalvqgswccz-5 0}ol.lst-kix_3tailnsur1sm-8.start{counter-reset:lst-ctn-kix_3tailnsur1sm-8 0}ol.lst-kix_gnobxi7p2flf-0.start{counter-reset:lst-ctn-kix_gnobxi7p2flf-0 0}.lst-kix_4uimhjzd4p6t-5>li{counter-increment:lst-ctn-kix_4uimhjzd4p6t-5}.lst-kix_km8gilkue97c-8>li{counter-increment:lst-ctn-kix_km8gilkue97c-8}ul.lst-kix_9ykvx5ecj10g-6{list-style-type:none}ol.lst-kix_7xxmatpvz3rf-0{list-style-type:none}ul.lst-kix_9ykvx5ecj10g-5{list-style-type:none}ul.lst-kix_9ykvx5ecj10g-8{list-style-type:none}ol.lst-kix_7xxmatpvz3rf-2{list-style-type:none}ol.lst-kix_km8gilkue97c-7.start{counter-reset:lst-ctn-kix_km8gilkue97c-7 0}ul.lst-kix_9ykvx5ecj10g-7{list-style-type:none}ol.lst-kix_7xxmatpvz3rf-1{list-style-type:none}ul.lst-kix_9ykvx5ecj10g-2{list-style-type:none}ol.lst-kix_7xxmatpvz3rf-4{list-style-type:none}ul.lst-kix_9ykvx5ecj10g-1{list-style-type:none}ol.lst-kix_7xxmatpvz3rf-3{list-style-type:none}ul.lst-kix_9ykvx5ecj10g-4{list-style-type:none}ol.lst-kix_7xxmatpvz3rf-6{list-style-type:none}ul.lst-kix_9ykvx5ecj10g-3{list-style-type:none}ol.lst-kix_7xxmatpvz3rf-5{list-style-type:none}ol.lst-kix_7xxmatpvz3rf-8{list-style-type:none}ol.lst-kix_7xxmatpvz3rf-7{list-style-type:none}ul.lst-kix_9ykvx5ecj10g-0{list-style-type:none}ol.lst-kix_rvalvqgswccz-4.start{counter-reset:lst-ctn-kix_rvalvqgswccz-4 0}ol.lst-kix_3tailnsur1sm-2.start{counter-reset:lst-ctn-kix_3tailnsur1sm-2 0}.lst-kix_rvalvqgswccz-7>li{counter-increment:lst-ctn-kix_rvalvqgswccz-7}.lst-kix_7xxmatpvz3rf-3>li{counter-increment:lst-ctn-kix_7xxmatpvz3rf-3}.lst-kix_mdcgjkjfaoe6-8>li{counter-increment:lst-ctn-kix_mdcgjkjfaoe6-8}.lst-kix_mdcgjkjfaoe6-0>li:before{content:"" counter(lst-ctn-kix_mdcgjkjfaoe6-0,decimal) ". "}.lst-kix_mdcgjkjfaoe6-7>li:before{content:"" counter(lst-ctn-kix_mdcgjkjfaoe6-7,lower-latin) ". "}ol.lst-kix_mdcgjkjfaoe6-1.start{counter-reset:lst-ctn-kix_mdcgjkjfaoe6-1 0}.lst-kix_mdcgjkjfaoe6-3>li:before{content:"" counter(lst-ctn-kix_mdcgjkjfaoe6-3,decimal) ". "}.lst-kix_mdcgjkjfaoe6-4>li:before{content:"" counter(lst-ctn-kix_mdcgjkjfaoe6-4,lower-latin) ". "}.lst-kix_4uimhjzd4p6t-5>li:before{content:"" counter(lst-ctn-kix_4uimhjzd4p6t-5,lower-roman) ". "}.lst-kix_cgm9v8hzfvyl-6>li:before{content:"\0025cf  "}ol.lst-kix_4uimhjzd4p6t-8.start{counter-reset:lst-ctn-kix_4uimhjzd4p6t-8 0}.lst-kix_4uimhjzd4p6t-1>li:before{content:"" counter(lst-ctn-kix_4uimhjzd4p6t-1,lower-latin) ". "}ol.lst-kix_km8gilkue97c-6.start{counter-reset:lst-ctn-kix_km8gilkue97c-6 0}ol.lst-kix_mdcgjkjfaoe6-7.start{counter-reset:lst-ctn-kix_mdcgjkjfaoe6-7 0}.lst-kix_cgm9v8hzfvyl-2>li:before{content:"\0025a0  "}.lst-kix_4firhkley64h-6>li{counter-increment:lst-ctn-kix_4firhkley64h-6}.lst-kix_9nztxlv7ylzk-2>li{counter-increment:lst-ctn-kix_9nztxlv7ylzk-2}ol.lst-kix_nylkdundlnhm-2{list-style-type:none}.lst-kix_gnobxi7p2flf-5>li:before{content:"(" counter(lst-ctn-kix_gnobxi7p2flf-5,lower-roman) ") "}ol.lst-kix_nylkdundlnhm-1{list-style-type:none}ol.lst-kix_nylkdundlnhm-0{list-style-type:none}ol.lst-kix_nylkdundlnhm-6{list-style-type:none}ol.lst-kix_nylkdundlnhm-5{list-style-type:none}ol.lst-kix_nylkdundlnhm-4{list-style-type:none}ol.lst-kix_nylkdundlnhm-3{list-style-type:none}.lst-kix_3tailnsur1sm-5>li{counter-increment:lst-ctn-kix_3tailnsur1sm-5}ol.lst-kix_nylkdundlnhm-8{list-style-type:none}ol.lst-kix_nylkdundlnhm-7{list-style-type:none}ol.lst-kix_km8gilkue97c-1.start{counter-reset:lst-ctn-kix_km8gilkue97c-1 0}ol.lst-kix_3tailnsur1sm-7.start{counter-reset:lst-ctn-kix_3tailnsur1sm-7 0}.lst-kix_km8gilkue97c-7>li:before{content:"" counter(lst-ctn-kix_km8gilkue97c-7,lower-latin) ". "}.lst-kix_7xxmatpvz3rf-2>li:before{content:"" counter(lst-ctn-kix_7xxmatpvz3rf-2,lower-roman) ". "}.lst-kix_mdcgjkjfaoe6-3>li{counter-increment:lst-ctn-kix_mdcgjkjfaoe6-3}.lst-kix_rvalvqgswccz-2>li{counter-increment:lst-ctn-kix_rvalvqgswccz-2}.lst-kix_9nztxlv7ylzk-4>li{counter-increment:lst-ctn-kix_9nztxlv7ylzk-4}.lst-kix_4uimhjzd4p6t-3>li{counter-increment:lst-ctn-kix_4uimhjzd4p6t-3}ol.lst-kix_rvalvqgswccz-6.start{counter-reset:lst-ctn-kix_rvalvqgswccz-6 0}.lst-kix_7xxmatpvz3rf-1>li{counter-increment:lst-ctn-kix_7xxmatpvz3rf-1}.lst-kix_gnobxi7p2flf-6>li{counter-increment:lst-ctn-kix_gnobxi7p2flf-6}.lst-kix_km8gilkue97c-3>li:before{content:"" counter(lst-ctn-kix_km8gilkue97c-3,decimal) ". "}.lst-kix_7xxmatpvz3rf-6>li:before{content:"" counter(lst-ctn-kix_7xxmatpvz3rf-6,decimal) ". "}ol.lst-kix_mdcgjkjfaoe6-5.start{counter-reset:lst-ctn-kix_mdcgjkjfaoe6-5 0}.lst-kix_7xxmatpvz3rf-8>li{counter-increment:lst-ctn-kix_7xxmatpvz3rf-8}ol.lst-kix_rvalvqgswccz-8.start{counter-reset:lst-ctn-kix_rvalvqgswccz-8 0}.lst-kix_9nztxlv7ylzk-7>li:before{content:"" counter(lst-ctn-kix_9nztxlv7ylzk-7,lower-latin) ". "}.lst-kix_km8gilkue97c-1>li{counter-increment:lst-ctn-kix_km8gilkue97c-1}.lst-kix_rvalvqgswccz-1>li:before{content:"" counter(lst-ctn-kix_rvalvqgswccz-1,lower-latin) ". "}ol.lst-kix_3tailnsur1sm-4.start{counter-reset:lst-ctn-kix_3tailnsur1sm-4 0}ol.lst-kix_km8gilkue97c-4.start{counter-reset:lst-ctn-kix_km8gilkue97c-4 0}.lst-kix_nylkdundlnhm-5>li{counter-increment:lst-ctn-kix_nylkdundlnhm-5}.lst-kix_9nztxlv7ylzk-3>li:before{content:"" counter(lst-ctn-kix_9nztxlv7ylzk-3,decimal) ". "}.lst-kix_nylkdundlnhm-3>li:before{content:"" counter(lst-ctn-kix_nylkdundlnhm-3,decimal) ". "}.lst-kix_mt7nux36tyir-3>li:before{content:"\0025cf  "}ol.lst-kix_km8gilkue97c-3.start{counter-reset:lst-ctn-kix_km8gilkue97c-3 0}.lst-kix_rvalvqgswccz-5>li:before{content:"" counter(lst-ctn-kix_rvalvqgswccz-5,lower-roman) ". "}.lst-kix_3tailnsur1sm-7>li:before{content:"" counter(lst-ctn-kix_3tailnsur1sm-7,lower-latin) ". "}ol.lst-kix_mdcgjkjfaoe6-4.start{counter-reset:lst-ctn-kix_mdcgjkjfaoe6-4 0}.lst-kix_nylkdundlnhm-7>li:before{content:"" counter(lst-ctn-kix_nylkdundlnhm-7,lower-latin) ". "}ol.lst-kix_3tailnsur1sm-5.start{counter-reset:lst-ctn-kix_3tailnsur1sm-5 0}.lst-kix_9ykvx5ecj10g-2>li:before{content:"\0025a0  "}.lst-kix_9ykvx5ecj10g-6>li:before{content:"\0025cf  "}.lst-kix_mt7nux36tyir-7>li:before{content:"\0025cb  "}.lst-kix_3tailnsur1sm-3>li:before{content:"(" counter(lst-ctn-kix_3tailnsur1sm-3,decimal) ") "}.lst-kix_3tailnsur1sm-1>li{counter-increment:lst-ctn-kix_3tailnsur1sm-1}ol.lst-kix_mdcgjkjfaoe6-3.start{counter-reset:lst-ctn-kix_mdcgjkjfaoe6-3 0}.lst-kix_gnobxi7p2flf-2>li{counter-increment:lst-ctn-kix_gnobxi7p2flf-2}ol.lst-kix_3tailnsur1sm-6.start{counter-reset:lst-ctn-kix_3tailnsur1sm-6 0}.lst-kix_7xxmatpvz3rf-6>li{counter-increment:lst-ctn-kix_7xxmatpvz3rf-6}ol.lst-kix_4uimhjzd4p6t-7.start{counter-reset:lst-ctn-kix_4uimhjzd4p6t-7 0}.lst-kix_4firhkley64h-8>li:before{content:"" counter(lst-ctn-kix_4firhkley64h-8,lower-roman) ". "}.lst-kix_km8gilkue97c-3>li{counter-increment:lst-ctn-kix_km8gilkue97c-3}ol.lst-kix_nylkdundlnhm-7.start{counter-reset:lst-ctn-kix_nylkdundlnhm-7 0}ol.lst-kix_km8gilkue97c-5.start{counter-reset:lst-ctn-kix_km8gilkue97c-5 0}ol.lst-kix_7xxmatpvz3rf-2.start{counter-reset:lst-ctn-kix_7xxmatpvz3rf-2 0}ul.lst-kix_lhvf5ffpxyq-8{list-style-type:none}.lst-kix_4uimhjzd4p6t-6>li{counter-increment:lst-ctn-kix_4uimhjzd4p6t-6}ul.lst-kix_lhvf5ffpxyq-2{list-style-type:none}ul.lst-kix_lhvf5ffpxyq-3{list-style-type:none}ul.lst-kix_lhvf5ffpxyq-0{list-style-type:none}ul.lst-kix_lhvf5ffpxyq-1{list-style-type:none}ul.lst-kix_lhvf5ffpxyq-6{list-style-type:none}ul.lst-kix_lhvf5ffpxyq-7{list-style-type:none}.lst-kix_4firhkley64h-1>li{counter-increment:lst-ctn-kix_4firhkley64h-1}ul.lst-kix_lhvf5ffpxyq-4{list-style-type:none}ul.lst-kix_lhvf5ffpxyq-5{list-style-type:none}ol.lst-kix_3tailnsur1sm-1.start{counter-reset:lst-ctn-kix_3tailnsur1sm-1 0}ol.lst-kix_mdcgjkjfaoe6-8{list-style-type:none}ol.lst-kix_gnobxi7p2flf-4.start{counter-reset:lst-ctn-kix_gnobxi7p2flf-4 0}ol.lst-kix_mdcgjkjfaoe6-7{list-style-type:none}ol.lst-kix_mdcgjkjfaoe6-6{list-style-type:none}ol.lst-kix_mdcgjkjfaoe6-5{list-style-type:none}ol.lst-kix_mdcgjkjfaoe6-4{list-style-type:none}ol.lst-kix_mdcgjkjfaoe6-3{list-style-type:none}ol.lst-kix_mdcgjkjfaoe6-8.start{counter-reset:lst-ctn-kix_mdcgjkjfaoe6-8 0}ol.lst-kix_4firhkley64h-6.start{counter-reset:lst-ctn-kix_4firhkley64h-6 0}ol.lst-kix_mdcgjkjfaoe6-2{list-style-type:none}ol.lst-kix_mdcgjkjfaoe6-1{list-style-type:none}ol.lst-kix_mdcgjkjfaoe6-0{list-style-type:none}ol.lst-kix_nylkdundlnhm-0.start{counter-reset:lst-ctn-kix_nylkdundlnhm-0 0}.lst-kix_4firhkley64h-7>li:before{content:"" counter(lst-ctn-kix_4firhkley64h-7,lower-latin) ". "}.lst-kix_9nztxlv7ylzk-5>li{counter-increment:lst-ctn-kix_9nztxlv7ylzk-5}.lst-kix_mdcgjkjfaoe6-5>li{counter-increment:lst-ctn-kix_mdcgjkjfaoe6-5}.lst-kix_4firhkley64h-5>li:before{content:"" counter(lst-ctn-kix_4firhkley64h-5,lower-roman) ". "}.lst-kix_4firhkley64h-6>li:before{content:"" counter(lst-ctn-kix_4firhkley64h-6,decimal) ". "}.lst-kix_4firhkley64h-3>li:before{content:"" counter(lst-ctn-kix_4firhkley64h-3,decimal) ". "}.lst-kix_4firhkley64h-4>li:before{content:"" counter(lst-ctn-kix_4firhkley64h-4,lower-latin) ". "}ol.lst-kix_4uimhjzd4p6t-0.start{counter-reset:lst-ctn-kix_4uimhjzd4p6t-0 1}ol.lst-kix_km8gilkue97c-0.start{counter-reset:lst-ctn-kix_km8gilkue97c-0 0}.lst-kix_4firhkley64h-0>li:before{content:"" counter(lst-ctn-kix_4firhkley64h-0,decimal) ". "}ol.lst-kix_7xxmatpvz3rf-7.start{counter-reset:lst-ctn-kix_7xxmatpvz3rf-7 0}.lst-kix_4firhkley64h-1>li:before{content:"" counter(lst-ctn-kix_4firhkley64h-1,lower-latin) ". "}.lst-kix_4firhkley64h-2>li:before{content:"" counter(lst-ctn-kix_4firhkley64h-2,lower-roman) ". "}.lst-kix_mdcgjkjfaoe6-1>li{counter-increment:lst-ctn-kix_mdcgjkjfaoe6-1}ol.lst-kix_rvalvqgswccz-3.start{counter-reset:lst-ctn-kix_rvalvqgswccz-3 0}.lst-kix_4uimhjzd4p6t-2>li{counter-increment:lst-ctn-kix_4uimhjzd4p6t-2}ol.lst-kix_4firhkley64h-1.start{counter-reset:lst-ctn-kix_4firhkley64h-1 0}ol.lst-kix_km8gilkue97c-0{list-style-type:none}ol.lst-kix_4firhkley64h-4.start{counter-reset:lst-ctn-kix_4firhkley64h-4 0}ol.lst-kix_9nztxlv7ylzk-5.start{counter-reset:lst-ctn-kix_9nztxlv7ylzk-5 0}ol.lst-kix_km8gilkue97c-8{list-style-type:none}ol.lst-kix_km8gilkue97c-7{list-style-type:none}.lst-kix_mt7nux36tyir-0>li:before{content:"\0025cf  "}ol.lst-kix_km8gilkue97c-6{list-style-type:none}ol.lst-kix_km8gilkue97c-5{list-style-type:none}ol.lst-kix_km8gilkue97c-4{list-style-type:none}ol.lst-kix_km8gilkue97c-3{list-style-type:none}.lst-kix_nylkdundlnhm-2>li{counter-increment:lst-ctn-kix_nylkdundlnhm-2}ol.lst-kix_nylkdundlnhm-5.start{counter-reset:lst-ctn-kix_nylkdundlnhm-5 0}ol.lst-kix_km8gilkue97c-2{list-style-type:none}ol.lst-kix_km8gilkue97c-1{list-style-type:none}ol.lst-kix_4uimhjzd4p6t-5.start{counter-reset:lst-ctn-kix_4uimhjzd4p6t-5 0}.lst-kix_nylkdundlnhm-1>li{counter-increment:lst-ctn-kix_nylkdundlnhm-1}.lst-kix_3tailnsur1sm-8>li{counter-increment:lst-ctn-kix_3tailnsur1sm-8}.lst-kix_9nztxlv7ylzk-8>li:before{content:"" counter(lst-ctn-kix_9nztxlv7ylzk-8,lower-roman) ". "}.lst-kix_nylkdundlnhm-0>li:before{content:"" counter(lst-ctn-kix_nylkdundlnhm-0,decimal) ". "}.lst-kix_rvalvqgswccz-6>li{counter-increment:lst-ctn-kix_rvalvqgswccz-6}ol.lst-kix_9nztxlv7ylzk-7.start{counter-reset:lst-ctn-kix_9nztxlv7ylzk-7 0}.lst-kix_9nztxlv7ylzk-6>li{counter-increment:lst-ctn-kix_9nztxlv7ylzk-6}.lst-kix_rvalvqgswccz-2>li:before{content:"" counter(lst-ctn-kix_rvalvqgswccz-2,lower-roman) ". "}.lst-kix_9nztxlv7ylzk-6>li:before{content:"" counter(lst-ctn-kix_9nztxlv7ylzk-6,decimal) ". "}.lst-kix_nylkdundlnhm-6>li:before{content:"" counter(lst-ctn-kix_nylkdundlnhm-6,decimal) ". "}ol.lst-kix_4uimhjzd4p6t-2.start{counter-reset:lst-ctn-kix_4uimhjzd4p6t-2 0}.lst-kix_9nztxlv7ylzk-2>li:before{content:"" counter(lst-ctn-kix_9nztxlv7ylzk-2,lower-roman) ". "}.lst-kix_9nztxlv7ylzk-4>li:before{content:"" counter(lst-ctn-kix_9nztxlv7ylzk-4,lower-latin) ". "}.lst-kix_nylkdundlnhm-2>li:before{content:"" counter(lst-ctn-kix_nylkdundlnhm-2,lower-roman) ". "}.lst-kix_nylkdundlnhm-4>li:before{content:"" counter(lst-ctn-kix_nylkdundlnhm-4,lower-latin) ". "}.lst-kix_rvalvqgswccz-0>li:before{content:"" counter(lst-ctn-kix_rvalvqgswccz-0,decimal) ". "}.lst-kix_mt7nux36tyir-4>li:before{content:"\0025cb  "}ol.lst-kix_nylkdundlnhm-2.start{counter-reset:lst-ctn-kix_nylkdundlnhm-2 0}.lst-kix_9nztxlv7ylzk-0>li:before{content:"" counter(lst-ctn-kix_9nztxlv7ylzk-0,decimal) ". "}ol.lst-kix_7xxmatpvz3rf-4.start{counter-reset:lst-ctn-kix_7xxmatpvz3rf-4 0}.lst-kix_mt7nux36tyir-2>li:before{content:"\0025a0  "}.lst-kix_mt7nux36tyir-6>li:before{content:"\0025cf  "}.lst-kix_rvalvqgswccz-6>li:before{content:"" counter(lst-ctn-kix_rvalvqgswccz-6,decimal) ". "}.lst-kix_4firhkley64h-8>li{counter-increment:lst-ctn-kix_4firhkley64h-8}.lst-kix_rvalvqgswccz-4>li:before{content:"" counter(lst-ctn-kix_rvalvqgswccz-4,lower-latin) ". "}ol.lst-kix_rvalvqgswccz-1.start{counter-reset:lst-ctn-kix_rvalvqgswccz-1 0}.lst-kix_nylkdundlnhm-8>li:before{content:"" counter(lst-ctn-kix_nylkdundlnhm-8,lower-roman) ". "}.lst-kix_mt7nux36tyir-8>li:before{content:"\0025a0  "}.lst-kix_rvalvqgswccz-8>li:before{content:"" counter(lst-ctn-kix_rvalvqgswccz-8,lower-roman) ". "}ol.lst-kix_gnobxi7p2flf-2.start{counter-reset:lst-ctn-kix_gnobxi7p2flf-2 0}.lst-kix_7xxmatpvz3rf-5>li{counter-increment:lst-ctn-kix_7xxmatpvz3rf-5}ul.lst-kix_50lonl88s381-5{list-style-type:none}ul.lst-kix_50lonl88s381-4{list-style-type:none}ul.lst-kix_50lonl88s381-7{list-style-type:none}ol.lst-kix_gnobxi7p2flf-6.start{counter-reset:lst-ctn-kix_gnobxi7p2flf-6 0}ul.lst-kix_50lonl88s381-6{list-style-type:none}ul.lst-kix_50lonl88s381-8{list-style-type:none}.lst-kix_3tailnsur1sm-2>li{counter-increment:lst-ctn-kix_3tailnsur1sm-2}.lst-kix_4uimhjzd4p6t-7>li{counter-increment:lst-ctn-kix_4uimhjzd4p6t-7}ul.lst-kix_50lonl88s381-1{list-style-type:none}ul.lst-kix_50lonl88s381-0{list-style-type:none}ul.lst-kix_50lonl88s381-3{list-style-type:none}ul.lst-kix_50lonl88s381-2{list-style-type:none}ol.lst-kix_9nztxlv7ylzk-3.start{counter-reset:lst-ctn-kix_9nztxlv7ylzk-3 0}.lst-kix_gnobxi7p2flf-3>li{counter-increment:lst-ctn-kix_gnobxi7p2flf-3}.lst-kix_lhvf5ffpxyq-6>li:before{content:"\0025cf  "}.lst-kix_lhvf5ffpxyq-7>li:before{content:"\0025cb  "}.lst-kix_mdcgjkjfaoe6-6>li{counter-increment:lst-ctn-kix_mdcgjkjfaoe6-6}.lst-kix_7xxmatpvz3rf-7>li{counter-increment:lst-ctn-kix_7xxmatpvz3rf-7}.lst-kix_lhvf5ffpxyq-2>li:before{content:"\0025a0  "}ol.lst-kix_nylkdundlnhm-4.start{counter-reset:lst-ctn-kix_nylkdundlnhm-4 0}ol.lst-kix_4firhkley64h-3.start{counter-reset:lst-ctn-kix_4firhkley64h-3 0}.lst-kix_lhvf5ffpxyq-3>li:before{content:"\0025cf  "}.lst-kix_9nztxlv7ylzk-1>li{counter-increment:lst-ctn-kix_9nztxlv7ylzk-1}.lst-kix_km8gilkue97c-2>li{counter-increment:lst-ctn-kix_km8gilkue97c-2}ol.lst-kix_7xxmatpvz3rf-0.start{counter-reset:lst-ctn-kix_7xxmatpvz3rf-0 0}ol.lst-kix_4firhkley64h-4{list-style-type:none}ol.lst-kix_4firhkley64h-3{list-style-type:none}ol.lst-kix_4firhkley64h-6{list-style-type:none}ol.lst-kix_4firhkley64h-5{list-style-type:none}.lst-kix_9nztxlv7ylzk-8>li{counter-increment:lst-ctn-kix_9nztxlv7ylzk-8}.lst-kix_mdcgjkjfaoe6-1>li:before{content:"" counter(lst-ctn-kix_mdcgjkjfaoe6-1,lower-latin) ". "}ol.lst-kix_4firhkley64h-0{list-style-type:none}.lst-kix_mdcgjkjfaoe6-2>li:before{content:"" counter(lst-ctn-kix_mdcgjkjfaoe6-2,lower-roman) ". "}ol.lst-kix_4firhkley64h-2{list-style-type:none}ol.lst-kix_4firhkley64h-1{list-style-type:none}.lst-kix_4firhkley64h-0>li{counter-increment:lst-ctn-kix_4firhkley64h-0}ol.lst-kix_4firhkley64h-8{list-style-type:none}ol.lst-kix_4firhkley64h-7{list-style-type:none}.lst-kix_nylkdundlnhm-6>li{counter-increment:lst-ctn-kix_nylkdundlnhm-6}.lst-kix_mdcgjkjfaoe6-5>li:before{content:"" counter(lst-ctn-kix_mdcgjkjfaoe6-5,lower-roman) ". "}ol.lst-kix_gnobxi7p2flf-5.start{counter-reset:lst-ctn-kix_gnobxi7p2flf-5 0}.lst-kix_mdcgjkjfaoe6-6>li:before{content:"" counter(lst-ctn-kix_mdcgjkjfaoe6-6,decimal) ". "}ol.lst-kix_9nztxlv7ylzk-4.start{counter-reset:lst-ctn-kix_9nztxlv7ylzk-4 0}.lst-kix_mdcgjkjfaoe6-2>li{counter-increment:lst-ctn-kix_mdcgjkjfaoe6-2}.lst-kix_cgm9v8hzfvyl-8>li:before{content:"\0025a0  "}ol.lst-kix_9nztxlv7ylzk-2.start{counter-reset:lst-ctn-kix_9nztxlv7ylzk-2 0}.lst-kix_rvalvqgswccz-3>li{counter-increment:lst-ctn-kix_rvalvqgswccz-3}.lst-kix_4uimhjzd4p6t-3>li:before{content:"" counter(lst-ctn-kix_4uimhjzd4p6t-3,decimal) ". "}ol.lst-kix_nylkdundlnhm-8.start{counter-reset:lst-ctn-kix_nylkdundlnhm-8 0}.lst-kix_cgm9v8hzfvyl-4>li:before{content:"\0025cb  "}.lst-kix_4uimhjzd4p6t-7>li:before{content:"" counter(lst-ctn-kix_4uimhjzd4p6t-7,lower-latin) ". "}.lst-kix_9nztxlv7ylzk-3>li{counter-increment:lst-ctn-kix_9nztxlv7ylzk-3}.lst-kix_cgm9v8hzfvyl-0>li:before{content:"\0025cf  "}.lst-kix_mt7nux36tyir-1>li:before{content:"\0025cb  "}ol.lst-kix_rvalvqgswccz-7{list-style-type:none}ol.lst-kix_rvalvqgswccz-6{list-style-type:none}ol.lst-kix_4firhkley64h-7.start{counter-reset:lst-ctn-kix_4firhkley64h-7 0}ol.lst-kix_rvalvqgswccz-8{list-style-type:none}.lst-kix_gnobxi7p2flf-3>li:before{content:"(" counter(lst-ctn-kix_gnobxi7p2flf-3,decimal) ") "}.lst-kix_gnobxi7p2flf-7>li:before{content:"" counter(lst-ctn-kix_gnobxi7p2flf-7,lower-latin) ". "}.lst-kix_km8gilkue97c-1>li:before{content:"" counter(lst-ctn-kix_km8gilkue97c-1,lower-latin) ". "}.lst-kix_km8gilkue97c-5>li:before{content:"" counter(lst-ctn-kix_km8gilkue97c-5,lower-roman) ". "}.lst-kix_7xxmatpvz3rf-8>li:before{content:"" counter(lst-ctn-kix_7xxmatpvz3rf-8,lower-roman) ". "}ol.lst-kix_rvalvqgswccz-1{list-style-type:none}ol.lst-kix_rvalvqgswccz-0{list-style-type:none}ol.lst-kix_rvalvqgswccz-3{list-style-type:none}ol.lst-kix_rvalvqgswccz-2{list-style-type:none}ol.lst-kix_rvalvqgswccz-5{list-style-type:none}ol.lst-kix_rvalvqgswccz-4{list-style-type:none}.lst-kix_nylkdundlnhm-4>li{counter-increment:lst-ctn-kix_nylkdundlnhm-4}.lst-kix_7xxmatpvz3rf-4>li:before{content:"" counter(lst-ctn-kix_7xxmatpvz3rf-4,lower-latin) ". "}ol.lst-kix_gnobxi7p2flf-7.start{counter-reset:lst-ctn-kix_gnobxi7p2flf-7 0}.lst-kix_km8gilkue97c-0>li{counter-increment:lst-ctn-kix_km8gilkue97c-0}.lst-kix_km8gilkue97c-7>li{counter-increment:lst-ctn-kix_km8gilkue97c-7}.lst-kix_7xxmatpvz3rf-2>li{counter-increment:lst-ctn-kix_7xxmatpvz3rf-2}.lst-kix_nylkdundlnhm-1>li:before{content:"" counter(lst-ctn-kix_nylkdundlnhm-1,lower-latin) ". "}.lst-kix_gnobxi7p2flf-5>li{counter-increment:lst-ctn-kix_gnobxi7p2flf-5}.lst-kix_9ykvx5ecj10g-0>li:before{content:"\0025cf  "}.lst-kix_rvalvqgswccz-3>li:before{content:"" counter(lst-ctn-kix_rvalvqgswccz-3,decimal) ". "}.lst-kix_9nztxlv7ylzk-5>li:before{content:"" counter(lst-ctn-kix_9nztxlv7ylzk-5,lower-roman) ". "}.lst-kix_nylkdundlnhm-5>li:before{content:"" counter(lst-ctn-kix_nylkdundlnhm-5,lower-roman) ". "}ol.lst-kix_3tailnsur1sm-7{list-style-type:none}ol.lst-kix_3tailnsur1sm-8{list-style-type:none}ol.lst-kix_3tailnsur1sm-5{list-style-type:none}ol.lst-kix_3tailnsur1sm-6{list-style-type:none}.lst-kix_4uimhjzd4p6t-4>li{counter-increment:lst-ctn-kix_4uimhjzd4p6t-4}ol.lst-kix_3tailnsur1sm-3{list-style-type:none}ol.lst-kix_3tailnsur1sm-4{list-style-type:none}.lst-kix_mdcgjkjfaoe6-4>li{counter-increment:lst-ctn-kix_mdcgjkjfaoe6-4}ol.lst-kix_3tailnsur1sm-1{list-style-type:none}ol.lst-kix_9nztxlv7ylzk-0.start{counter-reset:lst-ctn-kix_9nztxlv7ylzk-0 0}ol.lst-kix_3tailnsur1sm-2{list-style-type:none}ol.lst-kix_3tailnsur1sm-0{list-style-type:none}ol.lst-kix_gnobxi7p2flf-8.start{counter-reset:lst-ctn-kix_gnobxi7p2flf-8 0}ol.lst-kix_9nztxlv7ylzk-1.start{counter-reset:lst-ctn-kix_9nztxlv7ylzk-1 0}.lst-kix_9nztxlv7ylzk-1>li:before{content:"" counter(lst-ctn-kix_9nztxlv7ylzk-1,lower-latin) ". "}.lst-kix_9ykvx5ecj10g-8>li:before{content:"\0025a0  "}.lst-kix_mt7nux36tyir-5>li:before{content:"\0025a0  "}.lst-kix_4firhkley64h-5>li{counter-increment:lst-ctn-kix_4firhkley64h-5}.lst-kix_9ykvx5ecj10g-4>li:before{content:"\0025cb  "}.lst-kix_3tailnsur1sm-1>li:before{content:"" counter(lst-ctn-kix_3tailnsur1sm-1,lower-latin) ") "}.lst-kix_3tailnsur1sm-5>li:before{content:"(" counter(lst-ctn-kix_3tailnsur1sm-5,lower-roman) ") "}ol.lst-kix_4firhkley64h-8.start{counter-reset:lst-ctn-kix_4firhkley64h-8 0}.lst-kix_rvalvqgswccz-7>li:before{content:"" counter(lst-ctn-kix_rvalvqgswccz-7,lower-latin) ". "}.lst-kix_3tailnsur1sm-4>li{counter-increment:lst-ctn-kix_3tailnsur1sm-4}ol{margin:0;padding:0}table td,table th{padding:0}.c4{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Open Sans";font-style:normal}.c20{color:#333333;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c13{padding-top:10pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left;height:11pt}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center;height:11pt}.c22{margin-left:-0.8pt;padding-top:10pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c19{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:700;text-decoration:underline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Economica"}.c17{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c18{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Open Sans";font-style:normal}.c12{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c7{color:#ff0000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Open Sans";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c28{margin-left:-0.8pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c15{color:#ff0000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c38{padding-top:3pt;padding-bottom:6pt;line-height:1.5;page-break-after:avoid;text-align:left}.c2{margin-left:-0.8pt;padding-top:10pt;padding-bottom:0pt;line-height:1.5;text-align:left}.c10{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-indent:0.8pt;text-align:left}.c9{font-size:12pt;font-family:"Open Sans";color:#8c7252;font-weight:700}.c26{color:#000000;font-weight:400;font-size:16pt;font-family:"Arial"}.c29{color:#666666;font-weight:700;font-size:14pt;font-family:"Economica"}.c36{color:#000000;font-weight:700;font-size:24pt;font-family:"Economica"}.c30{color:#000000;font-weight:700;font-size:11pt;font-family:"Economica"}.c41{padding-top:10pt;padding-bottom:0pt;line-height:1.5;text-align:left}.c27{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c39{padding-top:10pt;padding-bottom:0pt;line-height:1.5;text-align:center}.c23{text-decoration:none;vertical-align:baseline;font-style:normal}.c16{background-color:#ffffff;max-width:495pt;padding:72pt 45pt 72pt 72pt}.c32{font-size:20pt;font-family:"Economica";font-weight:400}.c40{color:#8c7252;font-size:11pt}.c21{font-weight:400;font-family:"Open Sans"}.c43{color:#000000;font-size:13pt}.c35{orphans:2;widows:2}.c24{font-weight:700;font-family:"Open Sans"}.c6{color:inherit;text-decoration:inherit}.c37{color:#000000;font-size:16pt}.c25{padding:0;margin:0}.c33{color:#333333;font-size:11pt}.c34{height:14pt}.c31{margin-left:-0.8pt}.c42{color:#999999}.c11{margin-left:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c16"><p class="c28"><span class="c23 c29">15-618 Parallel Computer Architecture and Programming</span></p><p class="c10 title" id="h.mbjsiz6n6jlo"><span class="c23 c36">Meta Machine Learning: Hyperparameter optimization</span></p><p class="c10 title" id="h.wpcj23y9lqmg"><span class="c19"><a class="c6" href="https://www.google.com/url?q=https://pbollimp.github.io/Meta-Machine-Learning/&amp;sa=D&amp;ust=1544933007002000">https://pbollimp.github.io/Meta-Machine-Learning/</a></span></p><p class="c10 title" id="h.w2npf74vdqc2"><span class="c32">Priyatham Bollimpalli (pbollimp)<br>Mohit Deep Singh (mohitdes)</span></p><h2 class="c31 c38" id="h.7dqwglvp634w"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 4.00px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 4.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="horizontal line"></span></h2><h2 class="c2" id="h.mlerbatrbq1u"><span class="c26 c23">Abstract </span></h2><p class="c22"><span class="c21">In this project, we implemented various hyperparameter optimization algorithms primarily for Deep Neural Networks (Multi-Layer Perceptrons). We used OpenMP as the baseline for grid search, random search and a simple evolutionary algorithm. We then proceeded by implementing our own three variants of evolutionary algorithms using pthreads that not only give upto 9x speedups on 16 cores and scale very well with increasing number of threads, but also explore a much larger hyperparameter space and finds the hyperparameters that give good accuracy much faster than the baselines. Even though our experiments are specific to MLPs, we are confident that these methods are very applicable to hyperparameter optimization for any machine learning problem. We also performed an extensive breakdown of the execution times for our algorithms and reasoned about the performance of individual algorithms.</span></p><p class="c13 c31"><span class="c3"></span></p><h2 class="c2" id="h.vnn0p4mro164"><span>Background</span></h2><h3 class="c2" id="h.muv0b2hsipun"><span class="c9 c23">The task of Hyperparameter search </span></h3><p class="c5"><span class="c3">In the recent past, deep learning has been incredibly successful in various tasks such as image classification and tracking, machine translation, speech recognition etc. One of the biggest challenges faced by all machine learning researchers today (especially in deep learning) &nbsp;is selecting hyperparameters for models. A hyperparameter is a parameter whose value is set before the learning process starts. In contrast, model parameters are parameters that are optimized as part of the learning process. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">Hyperparameters usually have a huge impact on the amount of time it takes to train a model, and how well the model actually does. Usually, hyperparameters end up being critical in model performance, and that makes it critical for the researchers to find the most optimal values for it.</span></p><p class="c0"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 568.00px; height: 298.13px;"><img alt="" src="images/image9.png" style="width: 568.00px; height: 298.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c1">&nbsp;</span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">In practice today, most researchers manually tweak the hyperparameters and train the networks to see which hyperparameters give the best results. This process is extremely tedious and time consuming, and the process of automating this is called hyperparameter optimization. The few main approaches that exist as of today include:</span></p><p class="c0"><span class="c3"></span></p><ol class="c25 lst-kix_9nztxlv7ylzk-0 start" start="1"><li class="c4"><span class="c24">Grid Search</span><span class="c3">: This basically involves exhaustively searching through the search space of the hyperparameters, evaluating how each set of hyperparameters performs in terms of training, and choosing the best combination. This approach is extremely heavyweight, involves doing a brute force search over the space of hyperparameters. The advantage of this is that it can be parallelized well, and given enough resources (and good intuition), one can usually end up with the best set of hyperparameters.</span></li><li class="c4"><span class="c24">Random Search: </span><span class="c3">This approach involves randomly sampling hyperparameter sets through the search space, and searching till it gets the desired accuracy. This method is very successful in practice, since it doesn&rsquo;t exhaustively search the entire space, and still gets good results in practice.</span></li><li class="c4"><span class="c24">Bayesian Optimizations</span><span class="c3">: This approach usually involves using the past instances of the hyperparameters already tested to learn, and sample better sets of hyperparameters to evaluate. Evolutionary search and gaussian processes are the most common techniques used as bayesian optimization techniques. The biggest challenge is that since there is an optimization step, this makes these inherently sequential, and harder to parallelize.</span></li></ol><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><h3 class="c2" id="h.roan70j65zah"><span class="c9 c23">Evolutionary Search Algorithm</span></h3><p class="c5"><span class="c3">In the artificial intelligence literature, an evolutionary search algorithm is a generic population based meta-heuristic based optimization algorithm. An evolutionary strategy algorithm uses methods that are inspired by evolution, such a mutation, reproduction, reconstruction and selection. The high level idea is that given a population of individuals, we have a fitness function to rank which of the individuals are the best performers. Using this function, we rank the individuals, kill a certain fraction of the weakest part of the population, and have the stronger part of the population reproduce and mutate to arrive at better solutions.</span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c21">In our case, the individual hyperparameters sets are the candidates in a population. The accuracy of a neural network trained with those hyperparameters is our fitness function. The algorithm inputs are the population size, some initial random population of candidates and the mutations and crossovers mechanism. The output is the best candidate with the highest accuracy (fitness). The mutations can heavily vary based on the application. In our particular case,we randomly chose 3 candidates from the pool of the strongest candidates, took the average of their hyperparameters, and added some noise to it to encourage the algorithm to explore more.</span></p><p class="c0"><span class="c1"></span></p><h3 class="c2" id="h.lvyitpngnzhz"><span class="c9">Initial Analysis: Workload and Data Structures </span></h3><p class="c5"><span class="c3">The main components of our program are the neural network (training and inference), the dataset which the neural network is being trained on and the set of the best hyperparameters, or the hyperparameters that we are searching for. In the described algorithms, the main computationally expensive part is training and inferring multiple neural networks to get the accuracies of different sets of parameters. So parallelizing this would give us huge benefits, as we can test the fitness of multiple candidates in parallel. </span></p><p class="c5"><span class="c3">There are a few dependencies in this program. Firstly, for evolutionary search algorithms, we want to very carefully search through the hyperparameter search, so as to make sure we are regularly using the past results to carefully restrict our search space in the future to get better results. This requires us to frequently synchronize all parallel threads, evaluate the current population, rank it and mutate it. Furthermore, depending on what parameters we are searching, and what the values are, we could end up having very skewed workloads, where some hyperparameters based on their value might take a much longer time to train and evaluate as opposed to other. Therefore, these algorithms could greatly benefit from data parallelism in the first few approaches, where we could analyze fitness of the population in parallel, synchronize and then parallelize the mutations as well. </span></p><p class="c5"><span class="c21">The key data structures ended up being the neural networks, which we could not end up sharing (since we need to train each network individually through backpropagation to evaluate how well the hyperparameters would perform). The other key structure is the array of all candidates of a population or multiple populations. The main operations on this array were updating the correct accuracies, ranking them with respect to each other, and killing the weakest candidates and mutating and reproducing the strongest ones to produce new candidates. </span></p><h3 class="c2" id="h.a9iq0xr2t71c"><span class="c9 c23">Main Challenge</span></h3><p class="c0"><span class="c9 c23"></span></p><p class="c5"><span class="c3">The task of hyperparameter search is a very tedious one, especially given we need to train multiple neural networks from scratch with different hyperparameter configurations. As the networks become more complex, and the number of hyperparameters increase, this task becomes more and more challenging - the search space increases exponentially, and more applicable methods such as bayesian optimization techniques are difficult to parallelize. </span></p><p class="c5"><span class="c21">The main challenge we faced while trying to parallelize these algorithms was the ability to speed up not just the algorithms, but also get the correct tradeoff between the accuracy and how fast and effectively we can search the hyperparameter space. There is an inherently sequential part of any bayesian optimization, where based on the past results, we need to come up with better estimates of the hyperparameters. We tried parallelizing the algorithms in phases first. Then, we aimed to optimize that sequential part as well, and traded-off the quality of the bayesian optimization for a faster albeit slightly less accurate search.</span></p><p class="c13"><span class="c18"></span></p><h2 class="c2" id="h.hy09w2u5gwoo"><span>Approach</span></h2><p class="c35 c41"><span class="c3">In this section, we go into our approaches and techniques we used to parallelize the algorithms</span></p><h3 class="c2" id="h.dbkzvry04hht"><span class="c9 c23">Grid Search (GS)</span></h3><p class="c2 c35"><span class="c3">The high level algorithm is as follows:</span></p><p class="c35 c31 c39"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.00px; height: 246.67px;"><img alt="" src="images/image16.png" style="width: 660.00px; height: 246.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c35"><span class="c3">For grid search, we loop through all possible configurations of hyperparameters. In our case, the fitness function is training a neural network, and getting the accuracy of the network on the validation set. Parallelizing grid search was mostly straightforward, other than lines 5 and 6 in the algorithm. As a result, we just created an array that kept track of all accuracies for all configurations, and parallelized the calculation of the fitness for each possible configuration. In the end, we get the maximum accuracy, and select that configuration. We realized this was not very scalable with the size of the hyperparameters, so we broke up the entire parameter space into chunks, parallely calculate the fitness of those chunks, and incrementally keep track of the most accurate configuration.</span></p><p class="c0"><span class="c1"></span></p><h3 class="c2" id="h.1rnc83rynsmh"><span class="c9">Random Search (GS)</span></h3><p class="c2 c35"><span class="c21">The high level algorithm is as follows:</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.00px; height: 293.33px;"><img alt="" src="images/image13.png" style="width: 660.00px; height: 293.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">As can be seen in the algorithm, random search randomly samples hyperparameters from their search space, and tests them for their fitness. It keeps track on the best parameter configuration at any given point, and if the fitness is over a threshold, it stops searching the parameter space. Parallelizing this was not absolutely straightforward, but we followed the strategy of creating batches of threads which randomly sample from the hyperparameter space and report its accuracy to an array in the global memory. The master thread, every few chunks, gets the best accuracy from this array, and if it is over the threshold kills the program. Otherwise it keeps spawning off new blocks of threads to evaluate other random configurations. This is always in practice much faster than grid search, and parallelized version of it usually helps explore the space in a more efficient manner and get the desired accuracy pretty quickly.</span></p><h3 class="c2 c34" id="h.bbyo629gt1vv"><span class="c9 c23"></span></h3><h3 class="c2" id="h.3kirj8hzsmo"><span class="c9">Evolutionary Search - Basic Algorithm</span></h3><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.00px; height: 406.67px;"><img alt="" src="images/image11.png" style="width: 660.00px; height: 406.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">As can be seen in the algorithm, the basic evolutionary search works in a way where it tries to leverage the past evaluations of the fitness functions of the explored parameters to try and come up with a better and more effective form of optimizing the search space. </span></p><p class="c5"><span class="c3">The idea is the following. We start out with a population of candidate hyperparameters, and evaluate all of them. After evaluating them, we rank all the candidates in the population according to their fitness. We then kill/discard the last k candidates (least fit). Then we go ahead and generate new children using the top/best performing hyperparameter. The way we mutated our top performing parameters were: we randomly sample 3 hyperparameters from the top performing ones, and took the average. We then add some uniform noise to these averages to encourage our algorithm to explore. </span></p><p class="c0"><span class="c1"></span></p><h3 class="c2" id="h.s2ov7k99cm8n"><span class="c9 c23">Parallel Evolutionary Search 1 (EV1)</span></h3><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">The first approach we took was to try and parallelize the basic evolutionary search algorithm. We </span></p><p class="c5"><span class="c3">first decided to parallelize the evaluation of the fitness functions of the individual candidates. We ended up parallelizing this using OpenMP, and creating threads to divide this work up. We noticed that with certain hyperparameters like the number of hidden units, the workload imbalance ended up being very high, and openMP static scheduling did not give us good speedups. So we decided to use dynamic scheduling, and despite the existence of sequential code (which ranked the population and mutated it), we got decent speedups (discussed in the results section).</span></p><p class="c0"><span class="c1"></span></p><h3 class="c2" id="h.9sj0nm71bsme"><span class="c9 c23">Evolutionary Search with Islands - Our adaptation of the basic algorithm</span></h3><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c21">The earlier algorithm usually would do well with hyperparameter spaces as long as the dimensionality is not too high. But this algorithm is not ideal when the dimensionality of the number of hyperparameters we search for is high. Because of random sampling, if it does not start with good enough points, a basic evolutionary search would get stuck at local optimums. It helps if we create multiple local evolutionary search algorithm with specific start points, and then have them communicate with each other what their best parameters are after a few generations of local optimization. We call this variant the &ldquo;</span><span class="c24">island</span><span class="c3">&rdquo; approach, where you have island of local populations, which intermingle every few iterations. </span></p><p class="c5"><span class="c3">The basic idea was, we kept n different &ldquo;islands&rdquo;, each of which had its own population of candidate hyperparameters. Each of these islands separately carried out the evolutionary search on its candidate population for a few generations. This means each island would explore their candidates, rank them within their population and kill and repopulate the weakest members. After a few iterations, we took the top-k candidates from each of these islands, merged them together and applied a global evolution to it. This means, the merged candidates were now sorted, the weakest in this global evolutionary search were killed and repopulated based on the mutations explained in evolutionary search algorithm section. After this, we would reseed the local population with these globally mutated seed, and some random seeds, so that it can explore the high dimensional space in a better way.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c21">We explained how we parallelized these algorithms in the next few sections.</span></p><p class="c0"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.00px; height: 494.67px;"><img alt="" src="images/image2.png" style="width: 660.00px; height: 494.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c9 c23">Parallel Evolutionary Search 2 (EV2)</span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">We wanted to parallelize the island approach, to be able to scale better and search dense hyperparameter spaces more efficiently. We decided to use the fork join model with shared memory to implement this parallel approach. </span></p><p class="c5"><span class="c3">The initial idea was that the global top candidates would be stored in shared memory. The master then spawns off the requisite number of threads, and every thread essentially ends up acting as an island. Every island then reads its chunk of &ldquo;seed&rdquo; candidates from the global top candidates (which is shuffled after mutation by the master). Then each of these threads run a certain number of iterations of the evolutionary search algorithm locally. After finding its top candidates having run a few generations, the islands report their top-k candidates to the global array. The master waits for all these threads to complete executing their local &ldquo;generations&rdquo;, using join. Once every thread finishes, master then sorts these reported local winners in the global top array, and applies a &ldquo;global evolution&rdquo; on this array. This is followed by the master shuffling this global array, and then spawning off local threads again to do more iterations of these local evolutionary searches. </span></p><p class="c5"><span class="c21">The approach we took here was to parallelize the islands, rather than parallelizing every single candidate, so that it is easier for us to do a global intermingling of the best candidates. </span></p><h3 class="c2" id="h.qfle5km9q5cd"><span class="c9 c23">Parallel Evolutionary Search 3 (EV3)</span></h3><p class="c5"><span class="c3">Independent to EV2, we also tried implementing a threading model, where instead of using join every time, we used a pool of threads, and shared variables and mutexes to synchronize between the threads. The idea was to compare the performance of the two and see which one would do better. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">We used a similar model in terms of splitting up the islands between the threads. We had a shared count variables, and a mutex per thread, and also a mutex for the count variable. The master thread created a thread pool of the specified number of threads to begin with and would set the count variable to the number of threads. Each thread would work in an infinite loop, where at every iteration of the while loop, it would lock its respective mutex. Then it would get the requisite global &ldquo;seed&rdquo; candidates, and run the respective local evolutionary search algorithms. After each individual thread would be done, it would decrement the count variable by one, and loop back and try grabbing its own lock again (where it will wait). The master thread waits for the count variable to get to 0, and when it does, it sorts the global tops array, applies the respective global mutations, shuffles the array, and resets the count variable to num threads. After that the master thread unlocks the respective mutex for each thread so they can start running. This would run till either the master thread found an accuracy greater than the threshold, or a certain upper limit of iterations. Once the stopping condition would hit, the master thread sends out a signal to the other threads to quit, using a global variable. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">The motivation behind this was that, we thought pthread fork and joins may have extra overhead, and using mutexes might help us prevent that overhead. As discussed in the result section, this ended up being slower than EV2 for smaller number of threads, and that was mostly because of the synchronization overheads due to mutexes and shared variables. But for 16 threads, this outperformed EV2, because of the overhead of spawning new threads every iteration. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">Finally, we wanted to target and reduce the time the island program was taking in the sequential section. The approach we took is defined below.</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c9 c23"></span></p><p class="c5"><span class="c9 c23">Parallel Evolutionary Search 4 (EV4)</span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.00px; height: 494.67px;"><img alt="" src="images/image17.png" style="width: 660.00px; height: 494.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 255.50px; height: 274.87px;"><img alt="" src="images/image8.png" style="width: 255.50px; height: 274.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">In the last couple of approaches we noticed a few things. First was that sometimes the workload distribution ends up being skewed, and that causes some threads to just be idle to wait for other threads to catch up. Also, the global evolution would happen in a sequential fashion, which was definitely a bottleneck in further speedups. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">For this, we went back to the drawing board, and realized that the whole idea behind us coming up with the island approach was to have different local evolutionary populations communicate and intermingle to encourage more random exploration, and better evolution eventually. We realized that the communication does not need to be completely exact (and accurate) in time, and we could do this in an asynchronous manner. We built this on top of EV2, but got rid of the synchronizations. So every thread, including the master would exclusively work on its local population. We created three shared arrays of global populations. One of the arrays was called the &ldquo;read array&rdquo;: this is where the master initializes the first &ldquo;seed&rdquo; for all islands. The second array, was the &ldquo;write array&rdquo; where all threads would report the best candidates from their population. The third is the buffer array, which is used to do any intermittent calculations. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">The master first initializes the &ldquo;read array&rdquo;. Then it spawns off all other threads. Each thread basically runs the local evolutionary search algorithm, which either runs till the master signals it to stop (using shared variable), or it hits a certain number of iterations. The master also runs local evolutionary search, but every few iterations, it performs the global sorting and mutation step without blocking other threads. It copies the &ldquo;write array&rdquo; results to the buffer, sorts the buffer, applies global evolutionary search to it, and shuffles it. Then it switches the pointer locations of read array and the buffer, so now the threads can read the new globally evolved array. This allows all threads to operate asynchronously, where the only drawback is that the intermingling does not exactly happen at the same times for the threads. This is acceptable, because the main idea was to interchange information between islands, and by making this asynchronous we did not lose much accuracy. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">The only problem we faced with this was that the master thread now had to do extra work so it severely lagged behind, and that caused the other threads to work on stale data for a much longer time. As a fix to this, we significantly reduced the population size of the local evolutionary search for the master, and that way, the master was always either ahead of the other threads (which did us no harm), or would be at par with the other threads. We kept executing master till either it hit a threshold, or some other thread exited, which signaled all other threads to quit via a shared variable. This way, we completely removed any sequential code from our algorithm. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">We did tradeoff a little bit of freshness in terms of global evolution, but since the local evolutions follow the process correctly anyway, this ended up not causing a huge issue with accuracies. We just wanted information to flow through all threads to explore a larger space better, and not get stuck in local optimums, which this model helps us achieve. This gave us more speedups as we have shown in the results section.</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><h3 class="c2" id="h.96hb5awrupsu"><span class="c9">Parallel Evolutionary Search 5 (EV5)</span></h3><p class="c5"><span class="c3">For this approach we used the same model as EV4, but we also parallelized the part of the code where the master sorts and evolves the global array. We used openMP to implement a parallel for loop here and ended up parallelizing the evolutions carried out by the master thread. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c21">In addition to that, we also padded the three global arrays to completely take up the cache line size in order to prevent false sharing. This method thus is our best model in terms to performance and gave us the most speedups.</span></p><p class="c0"><span class="c1"></span></p><h2 class="c2" id="h.72mgnrcc57qz"><span class="c26 c23">Experimental Setup</span></h2><h3 class="c2" id="h.rq3bze3mmniy"><span class="c9 c23">Tiny DNN</span></h3><p class="c5"><span class="c3">Our initial approach was to use a highly optimized neural network library, to run hyperparameter search on convolutional neural networks. We wanted to stick with C/C++ for our implementation, as it is much easier to parallelize on that, rather than python (which has much better frameworks for deep learning). </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c21">We found a library called tiny-dnn which provided a very high level abstraction of neural networks. </span><span class="c21">It was a highly optimized library which could be linked to a project by using header only compilation</span><span class="c21">. This library was also in C++14 which prevented us from running this on GHC. We implemented most of our techniques using the Tiny DNN framework, but on actually running the tests on multiple threads, we saw that there were no performance improvements. We hypothesized that this is because of the tinyDNN doing some optimizations using openMP and pthreads under the hood in the background and this was clashing with our parallelization implementations. The library was taking up most resources on the limited resource machine we were running our experiments on. We could not profile because we could not run it on GHC (due to C++14) and AWS blocked proper profiling on their instances. At this point, we decided to abandon this library, and use a simpler DNN (a multi layer perceptron) to train our networks.</span></p><p class="c0"><span class="c1"></span></p><h3 class="c2" id="h.m04jupwsgp7o"><span class="c9 c23">MLP </span></h3><p class="c5"><span class="c3">We eventually decided that we will mostly try and use an existing simple multi-layer perceptron codebase, or reimplement our own version on a simple multilayer perceptron with backpropagation. We ended up taking some motivation and code [2], and reimplemented parts of it to fit our needs. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c21">This meant that we could not test our algorithms on very complicated networks (just MLPs), but given the resource constraints we had, and the amount of time it takes to train even a single complicated network, it would have taken us considerable amount of resources and time to write C++ code and train these networks to get good accuracies. So we scoped our problem down to multi-layer perceptrons, as it was easier for us to implement it and use a version of it for hyperparameter search. We still spent a considerable amount of time trying to implement and get a simple MLP working in C++, so we decided to focus the rest of our time trying to parallelize hyperparameter search, rather than build more complicated networks such as convolution networks.</span></p><p class="c0"><span class="c15"></span></p><h3 class="c2" id="h.4peuz5nwbva8"><span class="c9 c23">Datasets</span></h3><p class="c5"><span class="c3">We primarily tested our results on two datasets. We used the iris dataset as our first dataset, because it was a rather small classification dataset (150 points, with 3 labels), and it was very quick for us to train multiple networks in parallel with different hyperparameters to test the speedups and results. Once we verified our approaches and got some considerable speedups with the iris dataset, we switched to using the MNIST dataset, which was considerably larger (60,000 images). We initially wanted to get all our results on the CIFAR10 datasets, but since we resorted to using MLPs, it was very hard to get any substantial accuracies with the CIFAR dataset, and that was throwing our hyperparameter search off. </span></p><p class="c0"><span class="c3"></span></p><h3 class="c2" id="h.6n2wtkflql4q"><span class="c9 c23">Machine Specifications</span></h3><p class="c5"><span class="c21">All the experiments were run on the AWS c5.4xlarge instances. These instances have Intel Xeon Platinum 8000 series</span><span class="c23 c21 c33">, with 16 virtual cores and 32 gb memory. We also tested this on the GHC machine, which have eight 3.2 GHz Intel Core i7 processors, and we noticed similar speedups.</span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><p class="c0"><span class="c20"></span></p><h2 class="c2" id="h.oyu091klvyl4"><span class="c26 c23">Results and Analysis</span></h2><h3 class="c2" id="h.9y2uupbye5eo"><span class="c9 c23">Speedups</span></h3><p class="c5"><span class="c18">Wall Clock Times</span></p><p class="c5"><span class="c3">The wallclock time for different algorithms are given here. Observations:</span></p><ol class="c25 lst-kix_7xxmatpvz3rf-0 start" start="1"><li class="c4"><span class="c3">GS, RS, EV1 are going to be faster because the parameter space size on which they search is much less than the other algorithms. We significantly restricted these to measure speedups, and not waste time on running it on hyperparameter large spaces.</span></li><li class="c4"><span class="c3">EV3 takes the longest time since the synchronization cost is a lot more than the other techniques</span></li><li class="c4"><span class="c3">For random search, average of three runs is taken and since the algorithm stops when it reaches an given high accuracy threshold (75% for MNIST and 97% for Iris), it&rsquo;s total execution time is much less.</span></li><li class="c4"><span class="c3">EV4 execution time is less because it has near zero synchronization cost.</span></li><li class="c4"><span class="c3">EV5 execution time is the least among EV2-5s since it benefits the most from the parallelization while maintaining the same search space.</span></li></ol><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.50px; height: 380.15px;"><img alt="" src="images/image12.png" style="width: 601.50px; height: 380.53px; margin-left: 0.00px; margin-top: -0.19px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.00px; height: 417.33px;"><img alt="" src="images/image3.png" style="width: 660.00px; height: 417.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c24">Relative Speedups</span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">The relative speedup for different algorithms on both the datasets are given here. Observations:</span></p><ol class="c25 lst-kix_mdcgjkjfaoe6-0 start" start="1"><li class="c4"><span class="c3">RS doesn&rsquo;t scale after a point since the time needed for achieving a minimum good accuracy has been reached with a certain number of threads (8 in this case) and more threads do not improve the execution time since we hit the steady state in the expected iterations to hit the maximum accuracy threshold.</span></li><li class="c4"><span class="c3">EV 3, 4, 5 scales similarly since the parallelization technique is similar. EV 5 scales the best since it has no synchronization overhead.</span></li><li class="c4"><span class="c3">As the number of threads increase, the overhead of repeatedly creating pthreads increases for EV2 due to which it&rsquo;s speedup at the end is not as good as the others. EV3 just creates a pool of threads at the start and uses mutex to synchronize due to which it did not achieve perfect speed up but it did better than EV2. </span></li><li class="c4"><span class="c3">EV1 does not scale well since the contention between different threads increases as they are operating on the same array which leads to false sharing and more synchronization overhead due to some workload imbalance (explained in next section). Moreover, the search space (amount of work) is less, so more threads are not giving ideal speedups.</span></li><li class="c4"><span class="c3">EV5 scales the best since it has no synchronization (build from EV4), and additionally we added padding to the structure which reduces false sharing and it also contained additional parallelism in addition to the parallel component of parameter search due to which we get the best efficiency at the end.</span></li></ol><p class="c0 c11"><span class="c3"></span></p><p class="c5 c11"><span class="c3">To summarize, the overhead of creating threads, synchronization cost, part sequential work inherent in each algorithm in order to reconcile and get best hyperparameters and other costs (like initial data loading) prevent us from achieving perfect speedups. This is more elaborately portrayed for EV2, EV3 and EV4 in &lsquo;Normalized Execution Time breakdown&rsquo; section.</span></p><p class="c0 c11"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 606.50px; height: 382.74px;"><img alt="" src="images/image14.png" style="width: 606.50px; height: 383.70px; margin-left: 0.00px; margin-top: -0.48px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.00px; height: 417.33px;"><img alt="" src="images/image15.png" style="width: 660.30px; height: 417.33px; margin-left: -0.15px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c8"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><h3 class="c2" id="h.xknby2760d6g"><span class="c9 c23">Scaling with different parameters </span></h3><p class="c5"><span class="c3">This section gives the relative change in the execution time within each algorithm. The experiments are run using the best possible execution time that is achieved using 16 threads for all the algorithms.</span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c12">Dataset size</span></p><p class="c0"><span class="c12"></span></p><p class="c0"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.00px; height: 388.00px;"><img alt="" src="images/image7.png" style="width: 660.00px; height: 388.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">The relative execution time (degradation in this scale), when scaled with dataset size is is shown here. Note that the dataset sizes are increased exponentially, so an exponential slowdown in this case is observed (except for RS since the time to reach the accuracy threshold for the network becomes almost constant after certain amount of data). Since the data is passed through the neural network, irrespective of the algorithm, we should observe the same proportionate change in the execution times. This is because the forward pass, backward pass and the testing (which is 10% of the dataset) is done in the neural network and this changes exactly in the same way for all the algorithms due to which same trend is observed.</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c12">Number of Initial Hyperparameters</span></p><p class="c0"><span class="c12"></span></p><p class="c0"><span class="c1"></span></p><p class="c0 c11"><span class="c1"></span></p><p class="c0 c11"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.00px; height: 388.00px;"><img alt="" src="images/image1.png" style="width: 660.00px; height: 388.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">The relative execution time (degradation in this scale), when scaled with hyperparameter search size is shown here. There are multiple interesting observations. Note that the number of hyperparameters are also changed exponentially.</span></p><ol class="c25 lst-kix_rvalvqgswccz-0 start" start="1"><li class="c4"><span class="c3">The grid search is essentially an exhaustive search due to which an exponential increase in the hyperparameters gave an exponential increase in execution times.</span></li><li class="c4"><span class="c3">If the space of random hyperparameter sampling is more, it is observed that random search quickly finds the best model and reaches its accuracy threshold. Thus, the runtimes decrease in this case.</span></li><li class="c4"><span class="c3">The whole idea of evolutionary search is to retain only the best hyperparameters. Thus, even though we sample from a large hyper parameter space, since we discard the bad ones and only operate on the best ones, the execution time remains constant. There is a slight increase due to the increase in the sampling cost because of larger range.</span></li></ol><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0 c11"><span class="c1"></span></p><p class="c5"><span class="c12">Size of Neural Network</span></p><p class="c0"><span class="c12"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.00px; height: 388.00px;"><img alt="" src="images/image18.png" style="width: 660.00px; height: 388.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">The relative execution time (degradation in this scale), when scaled with the size of the neural network is shown here. Note that similar to the increase in the dataset, the cost incurred will be the same across all the algorithms. The training and testing times proportionately increase here too and since the depth of the network increases exponentially from 1 to 8, so are the relative execution times. </span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><h3 class="c2" id="h.i3dodt5c4zuf"><span class="c9 c23">Workload distribution between the threads in the algorithms</span></h3><p class="c0"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.00px; height: 369.33px;"><img alt="" src="images/image6.png" style="width: 660.00px; height: 369.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">The primary difference in EV2 and EV5 are that in EV2, the pthreads are forked and joined repeatedly. A major problem with this is the unequal execution times between the threads. For instance, having a hidden layer size of 512 would take more time to train and test compared to a 32 unit one. This would lead to multiple threads being idle waiting for the long running threads. In EV5, since the algorithm is modified to remove synchronization, all the threads can be busy and get the best result. From the figure above, for 16 threaded run, &lsquo;blue&rsquo; from EV5 are better distributed than the &lsquo;orange&rsquo; color ones from EV2. This can also be confirmed from the speedups and wallclock time graphs too in which EV5 is shown to be the best. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">Note that all the results on GS, RS and EV1 are reported on dynamically scheduled workload of openMP since similar trend is observed.</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><h3 class="c2" id="h.jt5wpnrtsxkn"><span class="c9">Normalized Execution Time Breakdown</span></h3><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">For each algorithm, a detailed normalized execution time breakdown is given below.</span></p><p class="c0"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 695.50px; height: 410.80px;"><img alt="" src="images/image5.png" style="width: 696.39px; height: 410.80px; margin-left: -0.44px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">The relative trends and speedups in the execution times are already explained above. But from this graph, we can clearly see exactly why that is the case. </span></p><p class="c5"><span class="c3">EV3 has a lot more synchronization cost associate with it since the threads stall a lot more to acquire the mutexes. This cost (time) adds up and only gets exponentially worse as the number of threads increase. </span></p><p class="c5"><span class="c3">EV2 in contrast, has some synchronization cost during the &lsquo;join&rsquo; part of the pthreads. But this is almost constant since the master executes them a fixed number of times (that is specific to the algorithm).</span></p><p class="c5"><span class="c3">We clearly see here that for EV4 (and EV5 too since its built on top of EV4), the lack of synchronization time makes them very efficient.</span></p><p class="c5"><span class="c3">The algorithms have similar proportion of total parallel work (given in orange) across the threads but we see the speedups because this is divided between the threads and each thread has lesser work to do as we increase the number of threads. </span></p><p class="c5"><span class="c21">Moreover, we can see that the inherent sequential part of the algorithms prevent us from achieving perfect speedup. </span></p><h3 class="c2" id="h.cigvtc1vp223"><span class="c9">&nbsp;Accuracy vs. Time</span></h3><p class="c0"><span class="c1"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 525.50px; height: 345.33px;"><img alt="" src="images/image10.png" style="width: 525.50px; height: 345.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c3">For this chart, we set a threshold for each of the parameters to stop searching the parameter space as soon as it hits a 75% accuracy (which is the a good accuracy on MNIST using MLP). We had significantly constrained the parameter space for grid search and random search to prevent long runtimes, so these numbers are the proportion of the time taken to get to the required accuracy, with respect to the total time taken(for an iteration bound search or an exhaustive search for that particular algorithm). </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">As we can see, grid search needs to do a sweep of all parameters before it can check the accuracies, so it takes the most time. Random search, has a threshold on accuracies that are checked after every block of parallel execution, and we can see that it reaches 75% accuracy in about 20% of the time it takes to do a longer sweep and still get similar results. </span></p><p class="c0"><span class="c3"></span></p><p class="c5"><span class="c3">We can see that EV4 takes the least amount of time, as it is searching in an asynchronous manner, and it still retains information across threads or islands. EV2 and EV3 are very close, and this can even be written down to the random initializations of the populations, but the idea is they get to a pretty good solution significantly faster than grid or random search. EV3 is slightly higher due to synchronization cost involved (explained in previous section)</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><h2 class="c2" id="h.1ks8zmp0vney"><span class="c26 c23">Credit Distribution</span></h2><p class="c5"><span class="c3">The work was equal by both partners. We both worked together on figuring out the approaches, setting up frameworks and getting a basic version of our project working. We split up the algorithms we have implemented equally, and had equal contribution in terms of the analysis and the report.</span></p><p class="c0"><span class="c1"></span></p><h2 class="c2" id="h.7hk0nqh0bplw"><span class="c26 c23">References</span></h2><p class="c0"><span class="c1"></span></p><p class="c5"><span>[1] </span><span class="c27"><a class="c6" href="https://www.google.com/url?q=https://github.com/tiny-dnn&amp;sa=D&amp;ust=1544933007050000">https://github.com/tiny-dnn</a></span></p><p class="c5"><span>[2] </span><span class="c27"><a class="c6" href="https://www.google.com/url?q=https://github.com/davidalbertonogueira/MLP/blob/master/README.md&amp;sa=D&amp;ust=1544933007050000">https://github.com/davidalbertonogueira/MLP/blob/master/README.md</a></span></p><p class="c5"><span>[3] </span><span class="c27"><a class="c6" href="https://www.google.com/url?q=https://deepmind.com/blog/population-based-training-neural-networks/&amp;sa=D&amp;ust=1544933007050000">https://deepmind.com/blog/population-based-training-neural-networks/</a></span></p><p class="c5"><span>[4] </span><span class="c27"><a class="c6" href="https://www.google.com/url?q=http://geco.mines.edu/workshop/aug2011/05fri/parallelGA.pdf&amp;sa=D&amp;ust=1544933007051000">http://geco.mines.edu/workshop/aug2011/05fri/parallelGA.pdf</a></span></p><p class="c5"><span>[5] </span><span class="c27"><a class="c6" href="https://www.google.com/url?q=https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/&amp;sa=D&amp;ust=1544933007051000">https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/</a></span></p><p class="c5"><span class="c1">Two images were taken from:</span></p><p class="c5"><span>[6] </span><span class="c27"><a class="c6" href="https://www.google.com/url?q=https://www.slideshare.net/alirezaandalib77/evolutionary-algorithms-69187390&amp;sa=D&amp;ust=1544933007052000">https://www.slideshare.net/alirezaandalib77/evolutionary-algorithms-69187390</a></span></p><p class="c5"><span>[7] </span><span class="c27"><a class="c6" href="https://www.google.com/url?q=https://www.slideshare.net/alirezaandalib77/evolutionary-algorithms-69187390&amp;sa=D&amp;ust=1544933007052000">https://www.slideshare.net/alirezaandalib77/evolutionary-algorithms-69187390</a></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p></body></html>